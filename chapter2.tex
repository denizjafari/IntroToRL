\documentclass[answers]{exam}
\usepackage{array}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{seqsplit}

% BASE TAKEN FROM ICCS315 SCRIBE NOTES

% --- SETUP STUFF ---
\usepackage[a4paper, margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{booktabs}

\usepackage{url}
\usepackage[unicode]{hyperref}

\setcounter{secnumdepth}{2}

\usepackage{titlesec,blindtext,color}

% --- MATH STUFF ---
\usepackage{amsthm, amsmath, amssymb}
\usepackage{mathtools,xspace}
\usepackage{nicefrac}

\usepackage{bbm}
\usepackage{dsfont}
\usepackage{cancel}

\usepackage{blkarray}
\newcommand{\matindex}[1]{\mbox{\scriptsize#1}} % Matrix index

% --- FONT STUFF ---
% Has to be under math stuff for some reason :/
\usepackage{newpxtext, newpxmath}
\usepackage[T1]{fontenc}

% --- DIAGRAM STUFF ---
\usepackage{tikz,pgfplots,xcolor,graphicx}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{caption}
\usepackage{subcaption}

\usepackage[breakable,skins]{tcolorbox}
\usepackage{framed}
\usepackage{mdframed}
\usepackage{float}

\pgfplotsset{compat=1.18}

% --- THEOREM STUFF ---
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\newtheorem{exercise}[theorem]{Exercise}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}

\usepackage{algorithm}
\usepackage[indLines=true]{algpseudocodex}
\usepackage{algorithmicx}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\Input{\item[\algorithmicinput]}
\algrenewcommand\algorithmicoutput{\textbf{Output:}}
\algrenewcommand\Output{\item[\algorithmicoutput]}
\algrenewcommand\algorithmicrequire{\textbf{Require:}}
\algrenewcommand\Require{\item[\algorithmicrequire]}

% --- CODE STUFF ---
\usepackage{minted}
\usemintedstyle{tango}
% from Stochastic Processes
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Proba}[1]{\mathbb{P}\left[#1\right]}
\newcommand{\CPr}[2]{\mathbf{Pr}\left[#1\ |\ #2\right]}
\newcommand{\Exp}{\mathbb{E}}
\newcommand{\ExSub}[2]{\mathbb{E}_#2\left[#1\right]}
\newcommand{\CEx}[2]{\mathbb{E}\left[#1\ |\ #2\right]}
\newcommand{\Expe}[1]{\mathbb{E}\left[ #1\right] }


\newcommand{\Pois}[1]{\text{Poisson}\left(#1\right)}
\newcommand{\DGamma}[2]{\text{Gamma}\left(#1, #2\right)}

% from Graph Theory
\DeclareMathOperator{\diam}{diam}

% misc. ones
\newenvironment{nexample}{
  \begin{leftbar}
    \noindent\textbf{Example.}
}{
  \end{leftbar}
}

\newenvironment{distraction}{
    \begin{tcolorbox}[title=Distraction!,breakable,colframe=red!69!black,before upper={\parindent15pt}]
}{
    \end{tcolorbox}
}

\newenvironment{hwproblem}[1]{\noindent\textbf{Problem #1.}}{}
\newmdenv[linewidth=0.5pt,linecolor=black,backgroundcolor=white]{singleframed}
\newenvironment*{singleframedindent}{
  \begin{singleframed}
    \setlength{\parindent}{\defparindent}\ignorespaces
  }{
  \end{singleframed}
}

\newcommand\sol[1]{
  \begin{singleframedindent}
    #1
  \end{singleframedindent}
}



\renewcommand{\questionlabel}{\textbf{Problem \thequestion.}}
\renewcommand{\solutiontitle}{}

\newcommand{\e}{\varepsilon}
\newcommand{\tor}{\text{ or }}

\begin{document}

\section{Chapter 1}

\section{Chapter 2}


\begin{exercise}
In "$\epsilon$-greedy action selection, for the case of two actions and " $\epsilon= 0.5$, what is
the probability that the greedy action is selected?
\end{exercise}
\begin{solution}
Write $G:=\{\text{greedy action is selected}\}$. We have
\begin{align*}
\Proba{\text{ G }}=&\Proba{\text{ G },random}+\Proba{\text{ G },optimal}    \\
&=\epsilon+(1-\epsilon)\frac{1}{2}\\
&=0.75.
\end{align*}  
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{exercise}
Bandit example Consider a $k$-armed bandit problem with $k = 4$ actions, denoted $1, 2, 3$, and $4$. Consider applying to this problem a bandit algorithm using $\e$-greedy action selection, sample-average action-value estimates, and initial estimates of $Q_1(a) = 0$, for all $a$. Suppose the initial sequence of actions and rewards is $A1 = 1, R1 = -1, A2 = 2, R2 = 1, A3 = 2, R3 = -2, A4 = 2, R4 = 2, A5 = 3, R5 = 0.$ On some of these time steps the $\e$ case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?
\end{exercise}
\begin{solution}


We denote the sample average action value after n-steps as
\begin{equation}
\Large{Q_t}(a) = \frac{{\sum\limits_{i = 1}^{t - 1} {{R_i}{1_{{A_i} = a}}} }}{{\sum\limits_{i = 1}^{t - 1} {{1_{{A_i} = a}}} }}.
\end{equation}

\begin{tabularx}{\linewidth}{ccccccc}
Timestep $t$ & $Q_{t+1}(1)$ & $Q_{t+1}(2)$    & $Q_{t+1}(3)$  & $Q_{t+1}(4)$ & Greedy action & Action selected\\
\hline 
t=0&0&0&0&0&-&$A_{1}=1$\\
\hline 
t=1&$\frac{R_{1}}{1_{{A_1} = 1}}=-1$  & 0 &0&0&$2,3\tor 4$&$A_{2}=2$\\
\hline 
t=2& $-1$ &1&0&0&2&$A_{3}=2$\\
\hline 
t=3& -1& $\frac{R_{2}+R_{3}}{2}=-0.5$ &0&0&3\tor 4&$A_{4}=2$\\
\hline 
t=4& -1 &$\frac{R_{2}+R_{3}+R_{4}}{3}=\frac{1}{3}$&0  &0&3&$A_{5}=3$\\
\hline 
t=5&-1&1/3&0&0&3&end\\
\hline
\end{tabularx}

\begin{enumerate}
    \item action $A_{1}=1$ can be either exploration or exploitation because the optimal value is zero for all actions.

    \item action $A_{2}=2$ could be either exploration or exploitation because action=2 is optimal too.

    \item action $A_{3}=2$ could be either exploration or exploitation because action=2 is optimal too.

\item action $A_{4}=2$ is exploration because the action 2 is not optimal.

\item action $A_{5}=4$ is exploitation because action=$3$ is the only optimal choice.
    
\end{enumerate}






\end{solution}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{exercise}
In the comparison shown in Figure 2.2, which method will perform best in the
long run in terms of cumulative reward and probability of selecting the best
action? How much better will it be? Express your answer quantitatively    
\end{exercise}

\begin{solution}
    
\textbf{ cumulative reward}
We have for optimal action $a_{*}$
\begin{equation}
\Expe{R_{i}|A_{i}=a_{*}}=q_{*}(a_{*}).    
\end{equation}
The probability of choosing the optimal action among n-options is
\begin{equation}
\Proba{A_{i}=a_{*}}=(1-\e)+\frac{\e}{n},
\end{equation}
and $\Proba{exploration}=1- \Proba{A_{i}=a_{*}}$. The true value $q_{*}(a)$ each of the ten actions $a=1,...,10$ was selected according to a normal distribution with mean zero and unit variance and so
\begin{equation}
\Expe{\Expe{R_{i}|exploration}}=\Expe{\sum_{action~a}\Expe{R_{i}|A_{i}=a}} =\sum_{action~a}\Expe{q_{*}(a)}=\sum_{action~a}0=0.   
\end{equation}
So the cumulative reward is from the law of total expectation
\begin{align*}
\Expe{R_{i}}=&\Expe{R_{i}|A_{i}=a_{*}}\Proba{A_{i}=a_{*}}+\Expe{R_{i}|exploration}\Proba{exploration}\\    
=&q_{*}(a_{*})((1-\e)+\frac{\e}{n})+\Expe{R_{i}|exploration}(\e-\frac{\e}{n})\\   
=&q_{*}(a_{*})((1-\e)+\frac{\e}{n}).
\end{align*}
We have
\begin{equation}
\Exp_{0.01}[R_{i}]=q_{*}(a_{*})0.991>q_{*}(a_{*})0.91=\Exp_{0.1}[R_{i}].   
\end{equation}
\textbf{probability of selecting the best action}
Write $G:=\{\text{greedy action is selected}\}$. We have for $\e=0.01$
\begin{align*}
\Prob_{0.01}(\text{ G })=&\Prob(\text{ G },random)+\Prob(\text{ G },optimal)    \\
&=\epsilon+(1-\epsilon)\frac{1}{k}\\
&=0.01+0.99\frac{1}{10}=0.991
\end{align*}  
and similarly for $\e=0.1$
\begin{align*}
Prob_{0.1}(\text{ G })=&Prob(\text{ G },random)+Prob(\text{ G },optimal)    \\
&=\epsilon+(1-\epsilon)\frac{1}{k}\\
&=0.1+0.9\frac{1}{10}=0.91
\end{align*}  
So we see that the first probability is slightly higher than the second one.



\end{solution}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}
    
\end{exercise}
\begin{solution}
    
\end{solution}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}
    
\end{exercise}
\begin{solution}
    
\end{solution}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}
    
\end{exercise}
\begin{solution}
    
\end{solution}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{exercise}
    
\end{exercise}
\begin{solution}
    
\end{solution}



\end{document}

